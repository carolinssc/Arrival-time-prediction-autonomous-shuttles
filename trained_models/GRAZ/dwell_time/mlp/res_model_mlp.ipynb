{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manity/SHOW_folder/SHOW_ML_Service/envs/show_env1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../../../../src')\n",
    "import os\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from data.datamodule import SHOWDataModule\n",
    "from models.gcn_model import NodeEncodedGCN, NodeEncodedGCN_tt, NodeEncodedGCN_1l, NodeEncodedGCN_2l, NodeEncodedGCN_3l\n",
    "\n",
    "\n",
    "def simple_model_evaluation(y_test, y_pred_test):\n",
    "    # Simple model evaluation that computes and prints MSE, RMSE and MAPE for the training and testing set\n",
    "\n",
    "    # train_error_mse = np.square(y_train - y_pred_train).sum() / y_train.shape[0]\n",
    "    test_error_mse = np.square(y_test - y_pred_test).sum() / y_test.shape[0]\n",
    "\n",
    "    # train_error_mape = (100 / y_train.shape[0]) * (\n",
    "    #    np.absolute(y_train - y_pred_train) / y_train\n",
    "    # ).sum()  # y_train should never be 0 since the travel time in a segment cannot be 0\n",
    "    test_error_mape = (100 / y_test.shape[0]) * (np.absolute(y_test - y_pred_test) / y_test).sum()\n",
    "\n",
    "    test_error_mae = (1 / y_test.shape[0]) * (np.absolute(y_test - y_pred_test)).sum()\n",
    "    print(\"-----------MSE----------\")\n",
    "    # print(\"Training error: {}\".format(train_error_mse))\n",
    "    print(\"Testing error: {}\".format(test_error_mse))\n",
    "    print(\"-----------RMSE----------\")\n",
    "    # print(\"Training error: {}\".format(np.sqrt(train_error_mse)))\n",
    "    print(\"Testing error: {}\".format(np.sqrt(test_error_mse)))\n",
    "    print(\"-----------MAPE----------\")\n",
    "    # print(\"Training error: {:.2f} %\".format(train_error_mape))\n",
    "    print(\"Testing error: {:.2f} %\".format(test_error_mape))\n",
    "    print(\"-----------MAE----------\")\n",
    "    print(\"Testing error: {}\".format(test_error_mae))\n",
    "    return test_error_mse, np.sqrt(test_error_mse), test_error_mape, test_error_mae\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the checkpoint directory\n",
    "checkpoint_dir = '.'\n",
    "model_names = os.listdir(checkpoint_dir)\n",
    "\n",
    "# remove all files not ending on ckpt\n",
    "model_names = [model_name for model_name in model_names if model_name.endswith('ckpt')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ne_gcn-3653-07271308.ckpt',\n",
       " 'ne_gcn-8353-07271308.ckpt',\n",
       " 'ne_gcn-7724-07271308.ckpt',\n",
       " 'ne_gcn-4015-07271308.ckpt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the datamodule\n",
    "site_name = 'GRAZ'\n",
    "transform = 'maxmin'\n",
    "batch_size = 64\n",
    "empty_graph = True\n",
    "time_kind = 'dwell_times'\n",
    "data_module = SHOWDataModule(\n",
    "    site_name=site_name,\n",
    "    transform=transform,\n",
    "    num_lags=2,\n",
    "    train_frac=0.9,\n",
    "    batch_size=batch_size,\n",
    "    empty_graph=empty_graph,\n",
    "    verbose=False,\n",
    "    time_kind=time_kind,\n",
    ")\n",
    "transform = data_module.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ne_gcn-3653-07271308.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manity/SHOW_folder/SHOW_ML_Service/envs/show_env1/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: /home/manity/SHOW_folder/SHOW_ML_Service/notebooks/trained_models2/GRAZ/dwell_time/mlp/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/manity/SHOW_folder/SHOW_ML_Service/envs/show_env1/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.89it/s]-----------MSE----------\n",
      "Testing error: 157.02899169921875\n",
      "-----------RMSE----------\n",
      "Testing error: 12.531121253967285\n",
      "-----------MAPE----------\n",
      "Testing error: inf %\n",
      "-----------MAE----------\n",
      "Testing error: 8.628461837768555\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manity/SHOW_folder/SHOW_ML_Service/envs/show_env1/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ne_gcn-8353-07271308.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/manity/SHOW_folder/SHOW_ML_Service/envs/show_env1/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 193.34it/s]-----------MSE----------\n",
      "Testing error: 154.5712890625\n",
      "-----------RMSE----------\n",
      "Testing error: 12.432670593261719\n",
      "-----------MAPE----------\n",
      "Testing error: inf %\n",
      "-----------MAE----------\n",
      "Testing error: 8.408819198608398\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 105.57it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manity/SHOW_folder/SHOW_ML_Service/envs/show_env1/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/manity/SHOW_folder/SHOW_ML_Service/envs/show_env1/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ne_gcn-7724-07271308.ckpt\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 210.63it/s]-----------MSE----------\n",
      "Testing error: 165.288818359375\n",
      "-----------RMSE----------\n",
      "Testing error: 12.856470108032227\n",
      "-----------MAPE----------\n",
      "Testing error: inf %\n",
      "-----------MAE----------\n",
      "Testing error: 8.85887622833252\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 126.42it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manity/SHOW_folder/SHOW_ML_Service/envs/show_env1/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/manity/SHOW_folder/SHOW_ML_Service/envs/show_env1/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ne_gcn-4015-07271308.ckpt\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 191.77it/s]-----------MSE----------\n",
      "Testing error: 169.37562561035156\n",
      "-----------RMSE----------\n",
      "Testing error: 13.014439582824707\n",
      "-----------MAPE----------\n",
      "Testing error: inf %\n",
      "-----------MAE----------\n",
      "Testing error: 9.134970664978027\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 120.77it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the models and evaluate them\n",
    "results = []\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "\n",
    "    # Load model checkpoint\n",
    "    checkpoint_dict = torch.load(model_name)\n",
    "    model = NodeEncodedGCN_1l(\n",
    "        transform=checkpoint_dict['hyper_parameters']['transform'],\n",
    "        weight_decay=checkpoint_dict['hyper_parameters']['weight_decay'],\n",
    "        lr=checkpoint_dict['hyper_parameters']['lr'],\n",
    "        drop_p=checkpoint_dict['hyper_parameters']['drop_p'],\n",
    "        batch_size=checkpoint_dict['hyper_parameters']['batch_size'],\n",
    "        input_size=checkpoint_dict['hyper_parameters']['input_size'],\n",
    "        hidden_layers=checkpoint_dict['hyper_parameters']['hidden_layers'],\n",
    "        aggregation_function=checkpoint_dict['hyper_parameters']['aggregation_function'],\n",
    "    )\n",
    "    model = model.load_from_checkpoint(f'{model_name}')\n",
    "\n",
    "    trainer = pl.Trainer(gpus=1)\n",
    "    results.append(trainer.test(model, datamodule=data_module, verbose=False))\n",
    "\n",
    "# Print the results\n",
    "mse_arr = np.array([run_dict[0]['test/error_mse'] for run_dict in results])\n",
    "mae_arr = np.array([run_dict[0]['test/error_mae'] for run_dict in results])\n",
    "mape_arr = np.array([run_dict[0]['test/error_mape'] for run_dict in results])\n",
    "rmse_arr = np.array([run_dict[0]['test/error_rmse'] for run_dict in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for GRAZ with <class 'data.datamodule.MaxMin'> transform and dwell_times time kind\n",
      "MSE: 161.56618118286133 +/- 6.007496441946299\n",
      "MAE: 8.757781982421875 +/- 0.269717478625642\n",
      "RMSE: 12.708675384521484 +/- 0.23613509869184718\n"
     ]
    }
   ],
   "source": [
    "print(f'Results for {site_name} with {type(transform)} transform and {time_kind} time kind')\n",
    "print(f'MSE: {mse_arr.mean()} +/- {mse_arr.std()}')\n",
    "print(f'MAE: {mae_arr.mean()} +/- {mae_arr.std()}')\n",
    "print(f'RMSE: {rmse_arr.mean()} +/- {rmse_arr.std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manity/SHOW_folder/SHOW_ML_Service/envs/show_env1/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/manity/SHOW_folder/SHOW_ML_Service/envs/show_env1/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ne_gcn-8353-07271308.ckpt\n",
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 19.48it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f11d0456b20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = model_names[np.argmin(mse_arr)]\n",
    "\n",
    "print(best_model)\n",
    "\n",
    "# Load model checkpoint\n",
    "checkpoint_dict = torch.load(best_model)\n",
    "model = NodeEncodedGCN_1l(\n",
    "    transform=checkpoint_dict['hyper_parameters']['transform'],\n",
    "    weight_decay=checkpoint_dict['hyper_parameters']['weight_decay'],\n",
    "    lr=checkpoint_dict['hyper_parameters']['lr'],\n",
    "    drop_p=checkpoint_dict['hyper_parameters']['drop_p'],\n",
    "    batch_size=checkpoint_dict['hyper_parameters']['batch_size'],\n",
    "    input_size=checkpoint_dict['hyper_parameters']['input_size'],\n",
    "    hidden_layers=checkpoint_dict['hyper_parameters']['hidden_layers'],\n",
    "    aggregation_function=checkpoint_dict['hyper_parameters']['aggregation_function'],\n",
    ")\n",
    "model = model.load_from_checkpoint(f'{best_model}')\n",
    "\n",
    "trainer = pl.Trainer(gpus=1)\n",
    "\n",
    "output = trainer.predict(model, dataloaders=data_module.test_dataloader())\n",
    "y_hat = np.concatenate([out[0] for out in output])\n",
    "y_true = np.concatenate([out[1] for out in output])\n",
    "\n",
    "df = pd.DataFrame({'y_true': y_true, 'y_hat': y_hat})\n",
    "df.to_csv(f'{best_model}_pred.csv', index=False)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(y_true, bins=100, alpha=0.75)\n",
    "ax.hist(y_hat, bins=100, alpha=0.75)\n",
    "ax.legend(['y_true', 'y_hat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "show_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
